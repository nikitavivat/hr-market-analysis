# Методология парсеров вакансий

## 1. HH.ru API Parser

### Технология
- **Метод**: Официальный REST API
- **Авторизация**: OAuth 2.0 (Bearer Token)
- **Библиотека**: `aiohttp` (асинхронные HTTP-запросы)

### Процесс работы

#### Шаг 1: Авторизация
- Получение `CLIENT_ID` и `CLIENT_SECRET` через dev.hh.ru
- OAuth 2.0 flow с редиректом на `http://127.0.0.1:5000/callback`
- Сохранение токена в `data/tokens/hh_token.json`
- Автоматическое обновление при истечении

#### Шаг 2: Поиск вакансий
- **Эндпоинт**: `GET /vacancies`
- **Параметры**: 
  - `text` - поисковый запрос (300+ профессий)
  - `area` - регион (113 = Россия)
  - `per_page` - 100 вакансий на страницу
  - `page` - номер страницы
- **Лимит**: до 40 страниц на запрос (~800 вакансий)

#### Шаг 3: Обработка данных
- **Параллелизм**: до 3 одновременных запросов (`MAX_CONCURRENT_REQUESTS`)
- **Задержки**: 
  - 0.2 сек между запросами
  - 0.3 сек между страницами
- **Данные из API** (без детальных запросов):
  - Название, компания, зарплата
  - Город, навыки, тип работы
  - Описание и требования (snippet)
  - Количество откликов

#### Шаг 4: Сохранение
- Запись в `data/raw/hh_vacancies.csv`
- Сохранение состояния в `data/scraper_state.json`
- Возобновление с последней страницы при прерывании

### Преимущества
- ✅ Высокая скорость (API)
- ✅ Надежность (официальный API)
- ✅ Нет блокировок
- ✅ Параллельная обработка

### Ограничения
- ⚠️ Требуется регистрация приложения
- ⚠️ Rate limiting (ограничение запросов)
- ⚠️ Нет полных описаний (только snippet)

---

## 2. LinkedIn Scraper

### Технология
- **Метод**: Браузерная автоматизация
- **Библиотека**: `Playwright` (Chromium)
- **Режим**: Видимый браузер (headless=False)

### Процесс работы

#### Шаг 1: Инициализация браузера
- Запуск Chrome через Playwright
- Создание persistent context (сохранение сессии)
- Блокировка изображений/стилей для ускорения
- Загрузка сохраненных cookies из `data/tokens/linkedin_cookies.json`

#### Шаг 2: Авторизация
- Открытие страницы входа LinkedIn
- **Ручной вход** пользователем
- Сохранение cookies после входа
- Проверка статуса авторизации

#### Шаг 3: Поиск вакансий
- Формирование URL: `https://www.linkedin.com/jobs/search/?keywords={query}`
- Прокрутка страницы для загрузки вакансий (lazy loading)
- Парсинг списка вакансий на странице
- Обход пагинации (до 10 страниц на запрос)

#### Шаг 4: Сбор детальной информации
- **Для каждой вакансии**:
  - Переход на страницу вакансии
  - Извлечение полного описания
  - Парсинг требований и навыков
  - Извлечение зарплаты (если указана)
- Обработка ошибок и пересоздание контекста при необходимости

#### Шаг 5: Извлечение данных
- **Автоматическое извлечение навыков** из текста (regex patterns)
- Парсинг зарплаты (поддержка USD, EUR, RUR)
- Очистка HTML тегов
- Нормализация данных

#### Шаг 6: Сохранение
- Запись в `data/raw/linkedin_vacancies.csv`
- Сохранение состояния в `data/linkedin_scraper_state.json`
- Сохранение cookies для следующего запуска

### Преимущества
- ✅ Полные описания вакансий
- ✅ Большой объем данных
- ✅ Сохранение сессии

### Ограничения
- ⚠️ Требуется ручной вход
- ⚠️ Медленнее чем API
- ⚠️ Риск блокировок
- ⚠️ Зависимость от структуры HTML

---

## 3. Rabota.ru Scraper

### Технология
- **Метод**: Браузерная автоматизация
- **Библиотека**: `Playwright` (Chromium)
- **Режим**: Headless (невидимый браузер)

### Процесс работы

#### Шаг 1: Инициализация
- Запуск Playwright в headless режиме
- Создание браузерного контекста
- Ротация User-Agent для избежания блокировок
- Инициализация CSV файла

#### Шаг 2: Загрузка обработанных URL
- Чтение существующего CSV
- Загрузка последних 5000 URL для проверки дубликатов
- Пропуск уже обработанных вакансий

#### Шаг 3: Поиск по профессиям
- **Список запросов**: 300+ профессий из `search_queries_300.py`
- Формирование URL: `https://www.rabota.ru/vacancy?query={profession}&page={page}`
- **Лимиты**:
  - До 600 вакансий на профессию
  - Максимум 20 страниц на профессию

#### Шаг 4: Парсинг страницы
- Ожидание загрузки контента (`domcontentloaded`)
- Принятие cookies (если требуется)
- Прокрутка страницы для lazy loading
- Поиск карточек вакансий через CSS селекторы

#### Шаг 5: Извлечение данных
- **Для каждой карточки**:
  - Название, компания, город
  - Зарплата (парсинг текста)
  - Тип работы
  - Краткое описание
- **Переход на детальную страницу**:
  - Полное описание
  - Требования
  - Дополнительная информация

#### Шаг 6: Обработка данных
- **Извлечение навыков**: regex patterns (Python, SQL, Java, etc.)
- **Парсинг зарплаты**: 
  - Диапазон "от-до"
  - Только "от" или "до"
  - Нормализация валюты (RUR)
- Очистка HTML тегов

#### Шаг 7: Сохранение
- Запись в `data/raw/rabota_vacancies.csv`
- Проверка дубликатов по URL
- Сохранение после каждой профессии
- Обработка профессий батчами (по 5)

### Преимущества
- ✅ Автоматическая работа (headless)
- ✅ Проверка дубликатов
- ✅ Батчевая обработка
- ✅ Обработка ошибок

### Ограничения
- ⚠️ Зависимость от структуры HTML
- ⚠️ Риск блокировок
- ⚠️ Медленнее чем API
- ⚠️ Требует стабильного интернета

---

## 4. GorodRabot Scraper

### Технология
- **Метод**: Браузерная автоматизация
- **Библиотека**: `Selenium WebDriver` (Chrome)
- **Парсинг**: `BeautifulSoup4` (HTML parsing)

### Процесс работы

#### Шаг 1: Инициализация
- Установка ChromeDriver через `webdriver-manager`
- Настройка Chrome options:
  - `--no-sandbox`
  - `--disable-dev-shm-usage`
- Создание WebDriver экземпляра

#### Шаг 2: Обход городов
- **50+ городов России** (Москва, СПб, Екатеринбург, etc.)
- Формирование URL: `https://{city-slug}.gorodrabot.ru/?p={page}`
- Пример: `https://moskva.gorodrabot.ru/?p=2`

#### Шаг 3: Парсинг страниц
- **Диапазон страниц**: 1-80 на город
- Открытие страницы через `driver.get(url)`
- Принятие cookies (если требуется)
- Ожидание загрузки контента (`WebDriverWait`)

#### Шаг 4: Извлечение данных
- **BeautifulSoup парсинг**:
  - Поиск элементов `div.vacancy.snippet`
  - Извлечение названия, компании, города
  - Парсинг зарплаты из текста
  - Извлечение типа работы и описания
- **Парсинг зарплаты**:
  - Поддержка форматов: "от X", "до Y", "X–Y"
  - Вычисление среднего значения
  - Нормализация чисел

#### Шаг 5: Задержки и паузы
- **Случайные задержки**: 3-6 секунд между страницами
- **Длинные паузы**: 25 секунд каждые 10 страниц
- Имитация человеческого поведения

#### Шаг 6: Сохранение
- Накопление всех вакансий в памяти
- Удаление дубликатов по (name, company, city, url)
- Сохранение в `src/EDA/gorodrabot_listings_v3.csv`
- Логирование в `list_scraper_v3.log`

### Преимущества
- ✅ Покрытие множества городов
- ✅ Простой и надежный парсинг
- ✅ Удаление дубликатов

### Ограничения
- ⚠️ Медленный (Selenium)
- ⚠️ Требует много памяти
- ⚠️ Нет детальных страниц
- ⚠️ Зависимость от структуры HTML

---

## Сравнительная таблица

| Параметр | HH.ru API | LinkedIn | Rabota.ru | GorodRabot |
|----------|-----------|----------|-----------|------------|
| **Метод** | REST API | Playwright | Playwright | Selenium |
| **Скорость** | ⚡⚡⚡ Очень быстро | ⚡⚡ Средне | ⚡⚡ Средне | ⚡ Медленно |
| **Надежность** | ✅✅✅ Высокая | ✅✅ Средняя | ✅✅ Средняя | ✅ Средняя |
| **Объем данных** | 800/профессия | Зависит | 600/профессия | 80 стр/город |
| **Авторизация** | OAuth 2.0 | Ручной вход | Не требуется | Не требуется |
| **Детальные данные** | ❌ Только snippet | ✅ Полные | ✅ Полные | ❌ Только snippet |
| **Параллелизм** | ✅ 3 потока | ❌ Последовательно | ❌ Последовательно | ❌ Последовательно |
| **Возобновление** | ✅ Да | ✅ Да | ✅ Да | ❌ Нет |
| **Риск блокировок** | ❌ Нет | ⚠️ Средний | ⚠️ Средний | ⚠️ Низкий |

---

## Общие принципы

### Обработка ошибок
- Retry логика с экспоненциальной задержкой
- Обработка таймаутов
- Логирование всех ошибок
- Продолжение работы при ошибках отдельных вакансий

### Управление состоянием
- Сохранение прогресса для возобновления
- Отслеживание обработанных вакансий
- Проверка дубликатов

### Оптимизация
- Блокировка ненужных ресурсов (изображения, стили)
- Параллельная обработка где возможно
- Батчевая обработка
- Кэширование cookies и токенов

### Данные
- Единый формат CSV для всех парсеров
- Нормализация полей
- Очистка HTML и лишних символов
- Извлечение навыков через regex

